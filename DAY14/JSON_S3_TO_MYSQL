import org.apache.spark.sql.{SparkSession,SaveMode}
import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}

object daily_task {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .master("local[5]")
      .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")
      .appName("Manish")
      .getOrCreate()

    val schema = StructType(List(
      StructField("id", IntegerType, nullable = true),
      StructField("first_name", StringType, nullable = true),
      StructField("last_name", StringType, nullable = true),
      StructField("age", IntegerType, nullable = true),
      StructField("email", StringType, nullable = true),
      StructField("position", StringType, nullable = true),
      StructField("salary", IntegerType, nullable = true)
    ))

    val hadoopConfig = spark.sparkContext.hadoopConfiguration
    hadoopConfig.set("fs.s3a.access.key", "aws.accessKeyId")
    hadoopConfig.set("fs.s3a.secret.key", "aws.secretAccessKey")
    hadoopConfig.set("fs.s3a.endpoint", "s3.amazonaws.com")
    hadoopConfig.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

    val employee_df = spark.read.schema(schema).json("s3a://s3-manishnitd/employee.json")

    val df_to_sql=employee_df.filter("age>30")

    df_to_sql.write
      .format("jdbc")
      .option("url", "jdbc:mysql://35.244.XX.XX:3306/manish")
      .option("dbtable", "employees")
      .option("user", "root")
      .option("password", "Password@12345")
      .mode(SaveMode.Overwrite) // Overwrite existing data
      .save()

    spark.stop()

}
}
