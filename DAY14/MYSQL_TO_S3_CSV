import org.apache.spark.sql.{SparkSession,SaveMode}
import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}

object daily_task1 {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .master("local[5]")
      .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")
      .appName("Manish")
      .getOrCreate()

    val hadoopConfig = spark.sparkContext.hadoopConfiguration
    hadoopConfig.set("fs.s3a.access.key", "aws.accessKeyId")
    hadoopConfig.set("fs.s3a.secret.key", "aws.secretAccessKey")
    hadoopConfig.set("fs.s3a.endpoint", "s3.amazonaws.com")
    hadoopConfig.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

    val employee_df = spark
      .read
      .format("jdbc")
      .option("url", "jdbc:mysql://35.244.19.XX:3306/manish")
      .option("dbtable", "employees")
      .option("user", "root")
      .option("password", "Password@12345")
      .load()


    val filtered_df=employee_df.filter("position='Project Manager'")

    filtered_df.write
      .format("csv")
      .mode(SaveMode.Overwrite)
      .save("s3a://s3-manishnitd/project_manager/")

    spark.stop()

  }
}
